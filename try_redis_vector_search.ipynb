{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "redis-stack\n",
      "7ae53e686a08734ec1c9988d5527fcb764aeee18416a995cad257efe435dce73\n"
     ]
    }
   ],
   "source": [
    "!docker rm redis-stack\n",
    "!docker run -d --cpus=1 --name redis-stack -p 5379:6379 -p 8001:8001 redis/redis-stack:latest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import redis\n",
    "r = redis.Redis(host='localhost', port=5379, db=0, protocol=3, decode_responses=True)\n",
    "r.ping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Allocated:  0.00287 GB\n",
      "Number of keys stored:  0\n"
     ]
    }
   ],
   "source": [
    "def print_redis_stats():\n",
    "    stats = r.execute_command(\"MEMORY STATS\")\n",
    "    print(f'Total Allocated: {stats[\"total.allocated\"]/2**30: .3} GB')\n",
    "    print(f'Number of keys stored: ', stats['keys.count'])\n",
    "    return stats\n",
    "_ = print_redis_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 100,000\n",
      "More than  0.61 GB  (size of embeddings only) will be stored in the table\n",
      "In fact, Redis requires ~55 MB per 1000 records. Which means, Redis will use ~5 GB of RAM.\n",
      "Also we need to reserve some memory for index\n"
     ]
    }
   ],
   "source": [
    "EMBEDDINGS_LEN = 1536  # Openai size\n",
    "\n",
    "NUM_DOCS = 10**5\n",
    "\n",
    "print(f'Number of documents: {NUM_DOCS:,}')\n",
    "print(f'More than {4*NUM_DOCS*EMBEDDINGS_LEN / 10**9: .2} GB  (size of embeddings only) will be stored in the table')\n",
    "\n",
    "print(f'In fact, Redis requires ~{(redis_100:=55)} MB per 1000 records. Which means, Redis will use ~{redis_100*NUM_DOCS//10**6} GB of RAM.')\n",
    "print('Also we need to reserve some memory for index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. clusters: 100\n",
      "Cluster size: 1000\n"
     ]
    }
   ],
   "source": [
    "NUM_CLUSTERS =  10**2\n",
    "CLUSTER_SIZE = int(NUM_DOCS / NUM_CLUSTERS)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "centers = np.random.rand(NUM_CLUSTERS,  EMBEDDINGS_LEN).astype(np.float32)\n",
    "print('Num. clusters:', centers.shape[0])\n",
    "errors = np.random.randn(CLUSTER_SIZE, EMBEDDINGS_LEN).astype(np.float32) / 20\n",
    "print('Cluster size:', errors.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility function, example of usage:\n",
      "next batch: (0, 1, 2, 3, 4, 5, 6, 7, 8, 9)\n",
      "next batch: (10, 11, 12, 13, 14, 15, 16, 17, 18, 19)\n",
      "next batch: (20, 21, 22, 23, 24, 25, 26, 27, 28, 29)\n",
      "next batch: (30, 31, 32, 33, 34, 35, 36, 37, 38, 39)\n",
      "next batch: (40, 41, 42, 43, 44, 45, 46, 47, 48, 49)\n",
      "next batch: (50, 51, 52, 53, 54, 55, 56, 57, 58, 59)\n",
      "next batch: (60, 61, 62, 63, 64, 65, 66, 67, 68, 69)\n",
      "next batch: (70, 71, 72, 73, 74, 75, 76, 77, 78, 79)\n",
      "next batch: (80, 81, 82, 83, 84, 85, 86, 87, 88, 89)\n",
      "next batch: (90, 91, 92, 93, 94)\n"
     ]
    }
   ],
   "source": [
    "from utils.batched import batched\n",
    "print('Utility function, example of usage:')\n",
    "for b in batched(range(95), batch_size=10): print('next batch:', b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for iterating over all docs:\n",
      "CPU times: user 61.1 ms, sys: 22.2 ms, total: 83.3 ms\n",
      "Wall time: 82.9 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def gen_data():\n",
    "    for i in range(NUM_CLUSTERS):\n",
    "        embeddings = (centers[i] + errors)#.tolist()\n",
    "        # idx = np.arange(i*CLUSTER_SIZE, (i+1)*CLUSTER_SIZE).astype(int)\n",
    "        idx = list(range(i*CLUSTER_SIZE, (i+1)*CLUSTER_SIZE ))\n",
    "\n",
    "\n",
    "        for j in range(CLUSTER_SIZE):\n",
    "            yield dict(\n",
    "                id=idx[j],\n",
    "                content=f'some unique content #{idx[j]}',\n",
    "                embedding=embeddings[j]\n",
    "            )\n",
    "\n",
    "print('Time for iterating over all docs:')\n",
    "%time for _ in gen_data(): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Allocated:  0.00287 GB\n",
      "Number of keys stored:  0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'peak.allocated': 6182764464,\n",
       " 'total.allocated': 3081800,\n",
       " 'startup.allocated': 1313200,\n",
       " 'replication.backlog': 0,\n",
       " 'clients.slaves': 0,\n",
       " 'clients.normal': 22760,\n",
       " 'cluster.links': 0,\n",
       " 'aof.buffer': 0,\n",
       " 'lua.caches': 0,\n",
       " 'functions.caches': 216,\n",
       " 'overhead.total': 1336176,\n",
       " 'keys.count': 0,\n",
       " 'keys.bytes-per-key': 0,\n",
       " 'dataset.bytes': 1745624,\n",
       " 'dataset.percentage': 98.70088958740234,\n",
       " 'peak.percentage': 0.04984501749277115,\n",
       " 'allocator.allocated': 3065328,\n",
       " 'allocator.active': 6691263488,\n",
       " 'allocator.resident': 6691263488,\n",
       " 'allocator-fragmentation.ratio': 2182.88671875,\n",
       " 'allocator-fragmentation.bytes': 6688198160,\n",
       " 'allocator-rss.ratio': 1.0,\n",
       " 'allocator-rss.bytes': 0,\n",
       " 'rss-overhead.ratio': 1.000004768371582,\n",
       " 'rss-overhead.bytes': 31744,\n",
       " 'fragmentation': 2182.89697265625,\n",
       " 'fragmentation.bytes': 6688229904}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean db\n",
    "r.flushall()\n",
    "print_redis_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Write docs to db: 100%|██████████| 100000/100000 [00:29<00:00, 3342.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.38 s, sys: 971 ms, total: 6.35 s\n",
      "Wall time: 29.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# from tqdm.notebook import trange, tqdm\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "from utils.numpy_to_json import FastJSONEncoder\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "# limit = 10_000\n",
    "limit = None\n",
    "\n",
    "\n",
    "doc_list = islice(\n",
    "    gen_data(),\n",
    "    limit\n",
    ")\n",
    "\n",
    "for batch in batched(\n",
    "                  tqdm(\n",
    "                      doc_list,\n",
    "                      total=limit or NUM_DOCS,\n",
    "                      desc='Write docs to db'\n",
    "                    ),\n",
    "              batch_size):\n",
    "    pipeline = r.pipeline()\n",
    "    for doc in batch:\n",
    "      \n",
    "      redis_key = f\"docs:{doc['id']}\"\n",
    "      \n",
    "      pipeline.json(encoder=FastJSONEncoder()).set(redis_key, \"$\", doc)\n",
    "    res = pipeline.execute()\n",
    "    assert all(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory footprint of Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Allocated:  5.63 GB\n",
      "Number of keys stored:  100000\n"
     ]
    }
   ],
   "source": [
    "_ = print_redis_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No index exist yet\n"
     ]
    }
   ],
   "source": [
    "INDEX_NAME = \"idx:docs_vss\"\n",
    "\n",
    "\n",
    "# Drop index if exists\n",
    "try:\n",
    "    r.ft(INDEX_NAME).info()\n",
    "except: print('No index exist yet')\n",
    "else: # no error happen - therefore index exists\n",
    "    print(\n",
    "        'Drop index:',\n",
    "        r.ft(INDEX_NAME).dropindex()\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 259 µs, sys: 664 µs, total: 923 µs\n",
      "Wall time: 9.56 ms\n"
     ]
    }
   ],
   "source": [
    "# %%time - no reason to measure time here - index creation will be running in background\n",
    "\n",
    "from redis.commands.search.field import TextField, NumericField, TagField, VectorField\n",
    "from redis.commands.search.indexDefinition import IndexDefinition, IndexType\n",
    "schema = (\n",
    "    TextField(\"$.content\", no_stem=True, as_name=\"content\"),\n",
    "    NumericField(\"$.id\", as_name=\"id\"),\n",
    "    VectorField(\n",
    "        \"$.embedding\",\n",
    "        \"FLAT\",\n",
    "        {\n",
    "            \"TYPE\": \"FLOAT32\",\n",
    "            \"DIM\": EMBEDDINGS_LEN,\n",
    "            \"DISTANCE_METRIC\": \"COSINE\",\n",
    "        },\n",
    "        as_name=\"vector\",\n",
    "    ),\n",
    ")\n",
    "definition = IndexDefinition(prefix=[\"docs:\"], index_type=IndexType.JSON)\n",
    "res = r.ft(INDEX_NAME).create_index(\n",
    "    fields=schema, definition=definition\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000.0 documents indexed with 0.0 failures\n"
     ]
    }
   ],
   "source": [
    "info = r.ft(INDEX_NAME).info()\n",
    "num_docs = info[\"num_docs\"]\n",
    "indexing_failures = info[\"hash_indexing_failures\"]\n",
    "print(f\"{num_docs} documents indexed with {indexing_failures} failures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index memory footprint\n",
    ">After index created, memory consumption was increased to 5.6 GB in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Allocated:  5.63 GB\n",
      "Number of keys stored:  100000\n"
     ]
    }
   ],
   "source": [
    "_ = print_redis_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic search (baseline performace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Retrieve by ID performance:\n",
      "718 µs ± 37.1 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "## Filter by numeric field performance:\n",
      "533 µs ± 36 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "## Result of search\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'docs:86712',\n",
       "  'extra_attributes': {'id': '86712', 'content': 'some unique content #86712'},\n",
       "  'values': []}]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "rid = random.randint(0, NUM_DOCS)\n",
    "\n",
    "print('## Retrieve by ID performance:')\n",
    "%timeit r.json().get(f\"docs:{rid}\")\n",
    "r.json().get(f\"docs:{rid}\")\n",
    "\n",
    "from redis.commands.search.query import Query, NumericFilter\n",
    "\n",
    "\n",
    "print('## Filter by numeric field performance:')\n",
    "query = Query(\"*\").add_filter(NumericFilter(field=\"id\", minval=rid, maxval=rid)).return_fields('id', 'content')\n",
    "%timeit r.ft(INDEX_NAME).search(query)['results']\n",
    "\n",
    "print('## Result of search')\n",
    "r.ft(INDEX_NAME).search(query)['results']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rid=12139\n",
      "156 ms ± 31.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "  vector_score     id  content\n",
      "--------------  -----  --------------------------\n",
      "    0.00324905  12386  some unique content #12386\n",
      "    0.00327367  12703  some unique content #12703\n",
      "    0.00329417  12131  some unique content #12131\n",
      "    0.00336385  12485  some unique content #12485\n",
      "    0.00336987  12468  some unique content #12468\n",
      "    0.00337225  12694  some unique content #12694\n",
      "    0.00339061  12483  some unique content #12483\n",
      "    0.00339288  12721  some unique content #12721\n",
      "    0.00339317  12036  some unique content #12036\n",
      "    0.00339681  12776  some unique content #12776\n"
     ]
    }
   ],
   "source": [
    "query = (\n",
    "    Query('(*)=>[KNN 10 @vector $query_vector AS vector_score]')\n",
    "     .sort_by('vector_score')\n",
    "     .return_fields('vector_score', 'id', 'content')\n",
    "     .dialect(2)\n",
    ")\n",
    "\n",
    "rid = random.randint(0, NUM_DOCS)\n",
    "print(f'{rid=}')\n",
    "\n",
    "rid_embeddings = np.array(centers[rid//CLUSTER_SIZE], dtype=np.float32)\n",
    "\n",
    "def search(embeddings: np.ndarray):\n",
    "    return r.ft(INDEX_NAME).search(query, { \n",
    "    'query_vector': embeddings.tobytes() \n",
    "})\n",
    "%timeit search(rid_embeddings)\n",
    "found = search(rid_embeddings)\n",
    "\n",
    "\n",
    "from tabulate import tabulate\n",
    "print(tabulate([d['extra_attributes'] for d in found['results']], headers='keys')) # type: ignore\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
